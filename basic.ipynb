{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Set up Asyncio**\n",
    "We need to handle async operations within our Jupyter Notebook environment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "import sys\n",
    "print(\"Python executable:\", sys.executable)\n",
    "print(\"Python path:\", sys.path)\n",
    "\n",
    "import numpy\n",
    "print(\"numpy version:\", numpy.__version__)\n",
    "print(\"Numpy path:\", numpy.__file__)\n",
    "\n",
    "import torch\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Set up Qdrant vector database**\n",
    "We'll use Qdrant as our vector database. Here we are going to store and retrieve vectore embeddings. \n",
    "\n",
    "Collections in Qdrant serve as the primary organisational unit for storing and managing vector data.\n",
    "Collections are designed to group similar or related vectors together, allowing for efficient search and retrieval operations within that group.\n",
    "\n",
    "Vectore requirements\n",
    "- all vectors within a collection must have the same dimensionality\n",
    "- a single distance metric is used for comparing vectors in a collection\n",
    "\n",
    "Supported distance metrics:\n",
    "- dot product\n",
    "- cosine\n",
    "- euclidean\n",
    "- manhattan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import qdrant_client\n",
    "\n",
    "collection_name = \"chat_with_docs\"\n",
    "\n",
    "client = qdrant_client.QdrantClient(\n",
    "  host=\"localhost\",\n",
    "  port=6333,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Read the documents**\n",
    "Load the document from the specified path and extract their contents for use in our RAG pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SimpleDirectoryReader\n",
    "\n",
    "input_dir_path = \"./docs\"\n",
    "\n",
    "loader = SimpleDirectoryReader(\n",
    "  input_dir=input_dir_path,\n",
    "  required_exts=[\".pdf\"],\n",
    "  recursive=True,\n",
    ")\n",
    "\n",
    "docs = loader.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(docs), len(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define function to index data**\n",
    "In this step I'm creating a function to create an index for our document embeddings, which will be store in the Qdrant vector database.\n",
    "The index will allow us to organise and search through the document embeddings efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex, StorageContext\n",
    "from llama_index.vector_stores.qdrant import QdrantVectorStore\n",
    "\n",
    "# this function converts each document into an embedding and stores them in the vector database\n",
    "def create_index(documents):\n",
    "  vector_store = QdrantVectorStore(client=client, collection_name=collection_name)\n",
    "\n",
    "  storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "\n",
    "  index = VectorStoreIndex.from_documents(documents, storage_context=storage_context)\n",
    "\n",
    "  return index\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Index our data**\n",
    "We are setting an embedding model from Hugging Face to convert our documents into vector embeddings, which we'll then store in Qdrant using the index function defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.core import Settings\n",
    "\n",
    "# the model used to generate the embeddings\n",
    "embed_model = HuggingFaceEmbedding(\n",
    "  model_name=\"BAAI/bge-large-en-v1.5\",\n",
    "  trust_remote_code=True,\n",
    "  device=\"cpu\",\n",
    ")\n",
    "\n",
    "# make sure the same model is used throughout the pipeline to maintain consistency in embeddings generation\n",
    "Settings.embed_model = embed_model\n",
    "\n",
    "index = create_index(docs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load the LLM**\n",
    "We configure an LLM to handle the response generation step in our RAG pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.ollama import Ollama\n",
    "\n",
    "# use the timeout parameter to prevent the LLM from hanging indefinitely\n",
    "llm = Ollama(model=\"llama3.2:1b\", request_timeout=120)\n",
    "\n",
    "# we set the LLM to be used throughout the pipeline\n",
    "Settings.llm = llm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define the prompt template**\n",
    "We create a prompt template that defines a consistent format to guide the LLM about the context it should look at while answering the query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import PromptTemplate\n",
    "\n",
    "template = \"\"\"Context information is below:\n",
    "              ---------------------\n",
    "              {context_str}\n",
    "              ---------------------\n",
    "              Given the context information above I want you to think\n",
    "              step by step to answer the query in a crisp manner,\n",
    "              incase you don't know the answer say 'I don't know!'\n",
    "            \n",
    "              Query: {query_str}\n",
    "        \n",
    "              Answer:\"\"\"\n",
    "\n",
    "# we define the prompt template\n",
    "prompt_template = PromptTemplate(\n",
    "  template=template,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Query the document**\n",
    "Finally, we utilize the index created above to set up a query engine which will use our indexed documents to process user queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = index.as_query_engine(\n",
    "  similarity_top_k=5,\n",
    "  response_mode=\"compact\",\n",
    "  verbose=True,\n",
    "  response_kwargs={\"answer_prefix\": \"Answer:\"},\n",
    ")\n",
    "\n",
    "query_engine.update_prompts(\n",
    "    {\"response_synthesizer:text_qa_template\": prompt_template}\n",
    ")\n",
    "\n",
    "response = query_engine.query(\"What exactly is DSPy?\")\n",
    "\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "display(Markdown(str(response)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-rag",
   "language": "python",
   "name": "learn-rag"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
